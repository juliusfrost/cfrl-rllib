driving-phase-1:
    run: PPO
    stop:
        time_total_s: 169200 # two days minus one hour in seconds
    checkpoint_freq: 1
    checkpoint_at_end: true
    keep_checkpoints_num: 1
    checkpoint_score_attr: episode_reward_mean
    local_dir: results 
    num_samples: 1
    log_to_file: log.txt
    config:
        framework: torch
        lambda: 0.95
        kl_coeff: 0.5
        clip_rewards: True
        clip_param: 0.1
        vf_clip_param: 10.0
        entropy_coeff: 0.01
        train_batch_size: 5000
        rollout_fragment_length: 100
        sgd_minibatch_size: 500
        num_sgd_iter: 10
        num_workers: 0
        num_envs_per_worker: 5
        batch_mode: truncate_episodes
        observation_filter: NoFilter
        vf_share_layers: true
        num_gpus: 1
        
        gamma: 0.999
        
        env: DrivingPLE-v0
        env_config:
            # reward function
            normalize_reward: False
            time_reward_proportion: 0
            reward_feature_weights: [-1., 0., -10., -1., 1., -0.1]
            collision_penalty: -10
            # state distribution
            prob_car: 0.5
            time_limit: 5000
            switch_prob: 0.01
            switch_duration: 100
            # action distribution
            action_noise: 0.
            steering_resistance: 100.
