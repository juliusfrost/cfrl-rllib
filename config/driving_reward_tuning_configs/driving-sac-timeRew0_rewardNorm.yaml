driving-sac:
    run: SAC
    config:
        gamma: 0.99
        # state-preprocessor=Our default Atari Conv2D-net.
        use_state_preprocessor: true
        Q_model:
            fcnet_activation: relu
            fcnet_hiddens: [128]
        policy_model:
            fcnet_activation: relu
            fcnet_hiddens: [128]
        # Do hard syncs.
        # Soft-syncs seem to work less reliably for discrete action spaces.
        tau: .01
        target_network_update_freq: 8000
        # auto = 0.98 * -log(1/|A|)
        target_entropy: auto
        clip_rewards: 1.0
        no_done_at_end: False
        n_step: 1
        rollout_fragment_length: 1
        prioritized_replay: true
        train_batch_size: 64
        timesteps_per_iteration: 4
        # Paper uses 20k random timesteps, which is not exactly the same, but
        # seems to work nevertheless. We use 100k here for the longer Atari
        # runs (DQN style: filling up the buffer a bit before learning).
#        learning_starts: 20000
        learning_starts: 500
        optimization:
            actor_learning_rate: 0.0003
            critic_learning_rate: 0.0003
            entropy_learning_rate: 0.0003
        num_workers: 0
        num_gpus: 1
        metrics_smoothing_episodes: 5
        env:  DrivingPLE-v0
        env_config:
            switch_prob: 0.0
            switch_duration: 50
            time_reward_proportion: 0
            COLLISION_PENALTY: -100
            normalize_reward: True
